* what is HAdoop
* what is a data lake
* what is a data swamp
* what is a data warehouse
* what is Hive
* what is meant by big data
* what is Yarn
* what is MapReduce
* what is HDFS
* what is batch processing
* what is pyspark
* what are the five Vs of big data
* what are the phases of MapReduce
* what is a node
* what is a reducer
* what is a data mart
* what is ELT
* what is ETL
* what is a data silo
* pros of data warehouse
* cons of data warehouse
* pros of data lake
* what are the two main frameworks for data processing
* cons of data lakes
* what is meant by commodity hardware
* how do data lakes fit into the existing data architecture
* advantages of the MapReduce approach
* what are the core components of Hadoop
* what is Hdfs
* what is meant by master-worker architecture
* what is oozie
* what is spark
* what is flume
* what is mahhout
* what is zookeepr
* what is pig
* what is the purpose of a resource manager
* how do prevent a data lake from becoming a data swamp
* which pyspark method is used to load data from a file (rdd)
* what does rdd stand for
* how do you create an rdd from a list
* what is flatMap()  (rdd)
* what rdd pyspark method can be used to transform each entry 
* why can't print be used by itself
* what is clustered computing
* what is parallel computing 
* what is distributed computing
* what is real-time processing
* how do you create an RDD from a data source
* what is batch processing
* what are the main ways of ingesting data
* what do dataframes allow us to do that rdds cannot
* what is the purpose of a lambda in pyspark (rdd)
* what is the purpose of reduceByKey()
* * what does filter() do pyspark (rdd)
* what is the entry point to Spark
* what is the purpose of having an entry point in spark
* what represents the entry point to Spark functionality
* where does the SparkContext get initiated
* how might you find the number of times that each word in a file occurs
* how do we create RDDs
* why are RDDs called RDDs - what are their characteristics
* what is a cluster
* what is a node
* what are lambdas often used with
* What is SparkContext
* what is lazy evaluation in RDDs[]
* what is data virtualization
* differences between data migration and data integration
* what are some data cleansing techniques
* what is data partitioning
* what is a partition in Spark
* are RDDs immutable or mutable
* what does getNumPartitions() do
 * what are the two different types of operations in PySpark
* what might be a use case for using flatMap() method of pyspark (rdd)
* what is the difference between map() and flatMap() (rdd)
* what does union() do
* what is meant by lazy evaluation
* what are the four actions()
* what is meant by pair RDDs
* what does groupByKey() do
* what does sortByKey() do
* what does reducebyKey() do
* what does join() do
* inner join
* left outer join
* right inner join
* what does reduce() do
* what does saveAsTextFile() do
* what does countByKey() action do
* what does collectAsMap() action to 
* actions vs transformations
* what made Hadoop popular - two features
* in which situations does batch processing work well
* what is list comprehension
* sortBy() vs sortByKey()
* how to sort a regular RDD in descending order
* how to sort a regular RDD in ascending order
* how to create a DataFrame from an RDD
* how do you create a DataFrame from a data source csv file
* how do you create a DataFrame from a .txt file
* how do you add a column
* what is one sorting method
* what is another sorting method
* these methods do what is seen typically in all programming languages where strings are sorted alphabetically, if numbers are strings they won't be sorted properly do need to be cast
* how to remove duplicate values from DFs
* how to group by a key
* how can you view the schema of a DataFram
* how can you get a summary of the data
* how can you view the data in a DataFrame - a couple of methods
* what does columns do
* how do you rename a column in a DF
* how do you extract a column or columns of a DF
* how to show the first ten rows
* what might data cleaning involve
* which method can be used to execute an SQL query
* can sql queries be run directly against a DF
* what is the purpose of createOrReplaceTempView()
* what is toPandas()
* what is pyspark_dist_explore
* what is HandySparkToPandas()
* what are the methods that are available within pyspark_dist_explore
* what are the benefits of converting a regular DF to a Pandas DF
* is it a good idea to use toPandas() when working with large volumes of data
* what are some main differences between Pandas dataframes and regular dataframes
* are Pandas dataframes mutable or immutable
* are regular dataframes mutable or immutable
* how do you convert a regular DF to a HandySpark DF
* how do we alias columns in DFs
* what does forEach() do
* what does forEachPartition() do
* what does distinct() do
* what method can be used to type cast columns aside from cast()
* how do you delete a column in a DF
* how do you remove null values/remove rows with null values
* how do you replace null values with a default value
* how do you replace values in DFs
* what does randomSplit() do
* what does sample() do
* which is fast Hadoop MapReduce or Spark
